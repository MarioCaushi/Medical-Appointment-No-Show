model_name,neurons,activation,optimizer,learning_rate,momentum,dropout,batch_size,f1,accuracy,precision,recall
Baseline MLP,"[128, 64]",relu,adam,0.001,0.0,0.3,32,0.7442,0.7237,0.6928,0.8046
Layers-1,[64],relu,adam,0.001,0.0,0.3,32,0.7324,0.711,0.682,0.7908
Layers-1,[128],relu,adam,0.001,0.0,0.3,32,0.7438,0.7227,0.6912,0.8051
Layers-2,"[64, 32]",relu,adam,0.001,0.0,0.3,32,0.7288,0.7139,0.6928,0.7688
Layers-2,"[128, 64]",relu,adam,0.001,0.0,0.3,32,0.7469,0.722,0.6854,0.8205
Layers-2,"[256, 128]",relu,adam,0.001,0.0,0.3,32,0.758,0.7334,0.6941,0.8349
Layers-3,"[128, 64, 32]",relu,adam,0.001,0.0,0.3,32,0.7459,0.7247,0.6926,0.8081
Layers-3,"[256, 128, 64]",relu,adam,0.001,0.0,0.3,32,0.7515,0.7344,0.7061,0.8032
Layers-3,"[512, 256, 128]",relu,adam,0.001,0.0,0.3,32,0.7533,0.7299,0.6933,0.8247
Activation-Relu,"[256, 128, 64]",relu,adam,0.001,0.0,0.3,32,0.7574,0.7383,0.7058,0.8172
Activation-Leakyrelu,"[256, 128, 64]",leakyrelu,adam,0.001,0.0,0.3,32,0.7539,0.7258,0.6838,0.8399
ADAM-LR0.001,"[256, 128, 64]",relu,adam,0.001,0.0,0.3,32,0.7578,0.7366,0.7014,0.824
ADAM-LR0.0005,"[256, 128, 64]",relu,adam,0.0005,0.0,0.3,32,0.7508,0.7322,0.702,0.8069
ADAM-LR0.0003,"[256, 128, 64]",relu,adam,0.0003,0.0,0.3,32,0.7533,0.7336,0.7015,0.8132
ADAM-LR0.0001,"[256, 128, 64]",relu,adam,0.0001,0.0,0.3,32,0.7559,0.7389,0.7095,0.8088
ADAMW-LR0.001,"[256, 128, 64]",relu,adamw,0.001,0.0,0.3,32,0.7508,0.7239,0.6842,0.8316
ADAMW-LR0.0005,"[256, 128, 64]",relu,adamw,0.0005,0.0,0.3,32,0.7454,0.7246,0.693,0.8064
ADAMW-LR0.0003,"[256, 128, 64]",relu,adamw,0.0003,0.0,0.3,32,0.738,0.7238,0.702,0.7779
ADAMW-LR0.0001,"[256, 128, 64]",relu,adamw,0.0001,0.0,0.3,32,0.7475,0.7228,0.6862,0.8209
SGD-LR0.01-M0.9,"[256, 128, 64]",relu,sgd,0.01,0.9,0.3,32,0.7457,0.7114,0.6665,0.8462
SGD-LR0.01-M0.95,"[256, 128, 64]",relu,sgd,0.01,0.95,0.3,32,0.7357,0.711,0.6778,0.8045
SGD-LR0.01-M0.99,"[256, 128, 64]",relu,sgd,0.01,0.99,0.3,32,0.7344,0.6698,0.6143,0.9127
SGD-LR0.005-M0.9,"[256, 128, 64]",relu,sgd,0.005,0.9,0.3,32,0.7383,0.7154,0.6833,0.8028
SGD-LR0.005-M0.95,"[256, 128, 64]",relu,sgd,0.005,0.95,0.3,32,0.7389,0.7145,0.6807,0.8079
SGD-LR0.005-M0.99,"[256, 128, 64]",relu,sgd,0.005,0.99,0.3,32,0.7347,0.6909,0.6434,0.8561
SGD-LR0.001-M0.9,"[256, 128, 64]",relu,sgd,0.001,0.9,0.3,32,0.7464,0.7273,0.6975,0.8027
SGD-LR0.001-M0.95,"[256, 128, 64]",relu,sgd,0.001,0.95,0.3,32,0.7421,0.7196,0.6869,0.8071
SGD-LR0.001-M0.99,"[256, 128, 64]",relu,sgd,0.001,0.99,0.3,32,0.7465,0.7275,0.6978,0.8026
Dropout - 0.2,"[256, 128, 64]",relu,adam,0.0001,0.0,0.2,32,0.7522,0.7347,0.7058,0.8051
Dropout - 0.5,"[256, 128, 64]",relu,adam,0.0001,0.0,0.5,32,0.7469,0.7322,0.708,0.7903
Batch-Size - 64,"[256, 128, 64]",relu,adam,0.0001,0.0,0.3,64,0.7548,0.7359,0.7044,0.8129
Batch-Size - 128,"[256, 128, 64]",relu,adam,0.0001,0.0,0.3,128,0.7591,0.741,0.7096,0.816
